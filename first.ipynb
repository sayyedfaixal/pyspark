{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the Pyspark with EXCEL\n",
    "1. Inorder to run the pyspark we have to create a Spark session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data_read = pd.read_csv('test.csv')\n",
    "data_read\n",
    "type(data_read)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here when we have open the csv using the pandas we are getting the type of data as pandas.core.frame.DataFrame, lets check what type we will be getting when we are reading the file using the pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Practice\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-0FSKS1T:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x104ee790>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|    _c0|_c1|\n",
      "+-------+---+\n",
      "|   Name|Age|\n",
      "| Faisal| 23|\n",
      "|  Imran| 28|\n",
      "| Zishan| 30|\n",
      "|Aayesha| 21|\n",
      "| Habiba| 20|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv('test.csv')\n",
    "df_pyspark.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are getting the column0 and column1 as the default columns, \n",
    "to get the header as name and age we have another function in spart \n",
    "Here the type of the data read from the Pyspark, is pyspark.sql.dataframe.DataFrame, whereas the data type from the pandas was pandas.core.frame.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "| Faisal| 23|\n",
      "|  Imran| 28|\n",
      "| Zishan| 30|\n",
      "|Aayesha| 21|\n",
      "| Habiba| 20|\n",
      "+-------+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Name='Faisal', Age='23'),\n",
       " Row(Name='Imran', Age='28'),\n",
       " Row(Name='Zishan', Age='30'),\n",
       " Row(Name='Aayesha', Age='21'),\n",
       " Row(Name='Habiba', Age='20')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark1 = spark.read.csv('test.csv')\n",
    "df_pyspark1 = spark.read.option('header', 'true').csv('test.csv')\n",
    "df_pyspark1.show()\n",
    "type(df_pyspark1)\n",
    "df_pyspark1.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame is a datastructure where we can perform various operations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "acc4ec3ac8ec90bb39b4725fe7319c4ac87533cd717104377e916c9391e3b5bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
